encoder_dataset:
  {encoder_dataset}
classifier_dataset:
  {classifier_dataset}
model:
  {model}
# n_layer:
#   - 4  # for CPAN
# heads:
#   - 1
# alpha:
#   - 0.2
data_ratio:
   - [0.8, 0.1, 0.1]
   - [0.1, 0.8, 0.1]
embedding_dim:
  - 128
  - 256
device:
  - cuda:0
batch_size:
  - 32
  - 128
learning_rate:
  - 0.0001
  - 0.001
encoder_epochs:
  - 0
  - 10000
classifier_epochs:
  - 10000
frozen_epochs:
  - [10, 10, 10, 10, 10]
  - [5, 10, 15, 20, 25]
hidden_units:
  - [64, 64, 64, 64]
optimizer:
  - Adam
scheduler:
  -
    class: StepLR
    args:
      step_size: 10
      gamma: 0.5
encoder_loss:
  - BCEWithLogitsLoss
classifier_loss:
  {classifier_loss}
metrics:
  - [Accuracy, ROC_AUC, F1, Average_precision]
train_eps:
  - true
l2:
  - 0.01
  - 0.001
aggregation:
  - mean
  - sum
gradient_clipping:
  - null
dropout:
  - 0.5
  - 0.2
  - 0.0
encoder_early_stopper:
  -
    class: Patience
    args:
      patience: 20
early_stopper:
  -
    class: Patience
    args:
      patience: 20
      monitor: val_Average_precision
      mode: max
shuffle:
  - True
resume:
  - False
