encoder_dataset:
  - [ZINC1k, JAK3]
  - [ZINC1k] 
  - [ZINC10k, JAK3]
  - [ZINC10k]
classifier_dataset:
  - JAK3Pre
  - JAK3
model:
  - GIN
  - CPAN
n_layer:
  - 4  # for CPAN
heads:
  - 1
mod:
  - origin
  - additive
  - scaled
use_encoder:
   - true
  #  - false
data_ratio:
   - [0.9, 0.1, 0]
   - [0.5, 0.5, 0]
   - [0.1, 0.9, 0]
embedding_dim:
  - 128
  # - 256
  # - 512
device:
  - cuda:0
batch_size:
  - 32
  # - 128
learning_rate:
  # - 0.01
  # - 0.001
  - 0.001
encoder_epochs:
  - 0
  - 10000
classifier_epochs:
  # - 100
  - 10000
frozen_epochs:
  # - 3
  - 10
hidden_units:  # Note: GIN add a first layer that simply adds up all node features
  - [64, 64, 64, 64]
  # - [32, 32, 32, 32]
  # - [64]
  # - [32, 32]
optimizer:
  - Adam
scheduler:
  -
    class: StepLR
    args:
      step_size: 10
      gamma: 0.5
encoder_loss:
  - BCEWithLogitsLoss
  # - MulticlassClassificationLoss
classifier_loss:
  - CrossEntropyLoss
metrics:
  - [Accuracy, ROC_AUC, F1, Average_precision]
train_eps:
  - true
  # - false
l2:
  - 0.
aggregation:
  - mean
  # - sum
gradient_clipping:
  - null
dropout:
  - 0.5
  # - 0.
encoder_early_stopper:
  -
    class: Patience
    args:
      patience: 20
      use_loss: True
early_stopper:
  -
    class: Patience
    args:
      patience: 20
      use_loss: False
  # -
  #   class: Patience
  #   args:
  #     patience: 20
  #     use_loss: True
shuffle:
  - True
resume:
  - False